{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Car Brand Classification with EfficientNet V2 and Grad-CAM\n\n## Mission Ready: Level 5 - Mission 1 (Bonus Challenge)\n\n### Objective\nTrain a deep learning model to classify car brands (33 classes) using transfer learning with EfficientNet V2, and visualize model decisions using Grad-CAM for explainability.\n\n### Target Metrics\n- **Validation Accuracy**: 80-90%+\n- **Test Accuracy**: 78-88%\n- **Training Time**: ~2-3 hours on Kaggle GPU\n\n### Dataset\n- **Source**: Car Brand Classification Dataset (ahmedelsany/Kaggle)\n- **Classes**: 33 car brands\n- **Total Images**: 16,467 images\n  - Train: 11,517 images (349 per brand)\n  - Validation: 2,475 images (75 per brand)\n  - Test: 2,475 images (75 per brand)\n- **Format**: Perfectly balanced classification dataset\n\n### Model Architecture\n- **Base Model**: EfficientNet V2 Medium (ImageNet-21k → ImageNet-1k pretrained)\n- **Framework**: PyTorch + TIMM\n- **Explainability**: Grad-CAM visualization\n\n### Brands Included\nAcura, Aston Martin, Audi, Bentley, BMW, Buick, Cadillac, Chevrolet, Chrysler, Dodge, FIAT, Ford, GMC, Honda, Hyundai, INFINITI, Jaguar, Jeep, Kia, Land Rover, Lexus, Lincoln, Mazda, Mercedes-Benz, MINI, Mitsubishi, Nissan, Porsche, Ram, Subaru, Toyota, Volkswagen, Volvo\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries with compatible versions\n!pip install --quiet timm==1.0.3 grad-cam 'numpy>=1.26,<2.0' 'scipy>=1.7,<1.15'\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\n\nimport timm\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport os\nimport json\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check for GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nelse:\n    print(\"WARNING: GPU not detected. Training will be very slow!\")\n    print(\"Please enable GPU in Kaggle settings: Accelerator → GPU T4 x2\")\n    \nprint(f\"\\nAll imports successful!\")\nprint(f\"timm version: {timm.__version__}\")\nprint(f\"numpy version: {np.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configuration\n",
    "\n",
    "All hyperparameters and paths are defined here for easy tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    # Paths (Kaggle environment)\n    'data_dir': '/kaggle/input/car-brand-classification-dataset/Car Brand Classification Dataset',\n    'output_dir': '/kaggle/working',\n    \n    # Model architecture\n    'model_name': 'tf_efficientnetv2_m.in21k_ft_in1k',\n    'pretrained': True,\n    \n    # Training hyperparameters\n    'batch_size': 32,\n    'num_epochs': 25,\n    'learning_rate': 1e-4,\n    'weight_decay': 1e-4,\n    'patience': 7,  # Early stopping patience\n    \n    # Data loading\n    'num_workers': 0,  # Set to 0 to avoid multiprocessing issues\n    'pin_memory': True,\n    \n    # Image preprocessing\n    'img_size': 224,\n    'mean': [0.485, 0.456, 0.406],\n    'std': [0.229, 0.224, 0.225],\n    \n    # Augmentation\n    'horizontal_flip_prob': 0.5,\n    'rotation_degrees': 15,\n    'color_jitter': (0.2, 0.2, 0.2, 0.1),  # brightness, contrast, saturation, hue\n    \n    # Scheduler\n    'scheduler_type': 'cosine',\n    'T_max': 25,  # For CosineAnnealingLR\n    'eta_min': 1e-6,\n}\n\nprint(\"Configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Dataset Exploration\n",
    "\n",
    "Let's explore the dataset structure and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "data_path = Path(CONFIG['data_dir'])\n",
    "print(f\"Dataset directory: {data_path}\")\n",
    "print(f\"Directory exists: {data_path.exists()}\")\n",
    "print()\n",
    "\n",
    "# List dataset structure\n",
    "if data_path.exists():\n",
    "    print(\"Dataset structure:\")\n",
    "    for item in sorted(data_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            num_items = len(list(item.iterdir()))\n",
    "            print(f\"  {item.name}/ ({num_items} items)\")\n",
    "        else:\n",
    "            print(f\"  {item.name}\")\n",
    "else:\n",
    "    print(\"⚠️ Dataset directory not found!\")\n",
    "    print(\"Please add the 'Car Brand Classification Dataset' via Kaggle's '+ Add Data' button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "def analyze_dataset_split(split_path):\n",
    "    \"\"\"Analyze a single dataset split (train/val/test)\"\"\"\n",
    "    if not split_path.exists():\n",
    "        return None, None\n",
    "    \n",
    "    class_counts = {}\n",
    "    for class_dir in sorted(split_path.iterdir()):\n",
    "        if class_dir.is_dir():\n",
    "            count = len(list(class_dir.glob('*.jpg'))) + len(list(class_dir.glob('*.png'))) + len(list(class_dir.glob('*.jpeg')))\n",
    "            class_counts[class_dir.name] = count\n",
    "    \n",
    "    return class_counts, sum(class_counts.values())\n",
    "\n",
    "# Check common dataset structures\n",
    "possible_structures = [\n",
    "    ('train', 'valid', 'test'),\n",
    "    ('train', 'val', 'test'),\n",
    "    ('train', 'test'),\n",
    "]\n",
    "\n",
    "found_structure = None\n",
    "for structure in possible_structures:\n",
    "    if all((data_path / split).exists() for split in structure[:2]):  # Check at least train and val/valid\n",
    "        found_structure = structure\n",
    "        break\n",
    "\n",
    "if found_structure:\n",
    "    print(f\"Found dataset structure: {found_structure}\\n\")\n",
    "    \n",
    "    for split in found_structure:\n",
    "        split_path = data_path / split\n",
    "        if split_path.exists():\n",
    "            class_counts, total = analyze_dataset_split(split_path)\n",
    "            print(f\"{split.upper()} SET:\")\n",
    "            print(f\"  Total images: {total}\")\n",
    "            print(f\"  Number of brands: {len(class_counts)}\")\n",
    "            if class_counts:\n",
    "                print(f\"  Brands: {', '.join(sorted(class_counts.keys())[:10])}...\" if len(class_counts) > 10 else f\"  Brands: {', '.join(sorted(class_counts.keys()))}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"⚠️ Could not determine dataset structure\")\n",
    "    print(\"Available directories:\")\n",
    "    for item in sorted(data_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Transforms and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=CONFIG['horizontal_flip_prob']),\n",
    "    transforms.RandomRotation(CONFIG['rotation_degrees']),\n",
    "    transforms.ColorJitter(*CONFIG['color_jitter']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIG['mean'], std=CONFIG['std'])\n",
    "])\n",
    "\n",
    "print(\"Training augmentations:\")\n",
    "print(\"  - Random horizontal flip\")\n",
    "print(f\"  - Random rotation (±{CONFIG['rotation_degrees']}°)\")\n",
    "print(\"  - Color jitter (brightness, contrast, saturation, hue)\")\n",
    "print(\"\\nValidation/Test: Resize + Normalize only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Load Datasets\n",
    "\n",
    "Load the train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine actual split names from dataset\n",
    "if found_structure:\n",
    "    train_split, val_split = found_structure[0], found_structure[1]\n",
    "    test_split = found_structure[2] if len(found_structure) > 2 else None\n",
    "else:\n",
    "    # Default assumption\n",
    "    train_split, val_split, test_split = 'train', 'valid', 'test'\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageFolder(root=str(data_path / train_split), transform=train_transform)\n",
    "val_dataset = ImageFolder(root=str(data_path / val_split), transform=val_transform)\n",
    "\n",
    "if test_split and (data_path / test_split).exists():\n",
    "    test_dataset = ImageFolder(root=str(data_path / test_split), transform=val_transform)\n",
    "    print(f\"Test dataset loaded: {len(test_dataset)} images\")\n",
    "else:\n",
    "    test_dataset = None\n",
    "    print(\"No test dataset found - will use validation set for final evaluation\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=CONFIG['pin_memory']\n",
    ")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        pin_memory=CONFIG['pin_memory']\n",
    "    )\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Number of brands: {num_classes}\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "if test_dataset:\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"\\nBrands: {', '.join(class_names[:10])}...\" if num_classes > 10 else f\"\\nBrands: {', '.join(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    mean = torch.tensor(CONFIG['mean']).view(3, 1, 1)\n",
    "    std = torch.tensor(CONFIG['std']).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Visualize random samples from training set\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('Sample Training Images (with augmentation)', fontsize=16, y=0.98)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img, label = train_dataset[np.random.randint(len(train_dataset))]\n",
    "    img_denorm = denormalize(img).permute(1, 2, 0).numpy()\n",
    "    img_denorm = np.clip(img_denorm, 0, 1)\n",
    "    \n",
    "    ax.imshow(img_denorm)\n",
    "    ax.set_title(f\"{class_names[label]}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'sample_images_brand.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample images saved to {CONFIG['output_dir']}/sample_images_brand.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Architecture\n",
    "\n",
    "Create EfficientNet V2 model with custom classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = timm.create_model(\n",
    "    CONFIG['model_name'],\n",
    "    pretrained=CONFIG['pretrained'],\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Input size: {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"Output classes: {num_classes}\")\n",
    "print(f\"Pretrained: {CONFIG['pretrained']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Training Setup\n",
    "\n",
    "Define loss function, optimizer, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if CONFIG['scheduler_type'] == 'cosine':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['T_max'],\n",
    "        eta_min=CONFIG['eta_min']\n",
    "    )\n",
    "else:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3\n",
    "    )\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: AdamW (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "print(f\"  Scheduler: {CONFIG['scheduler_type']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['patience']} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    pbar = tqdm(loader, desc='Training')\n    for inputs, labels in pbar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        pbar.set_postfix({\n            'loss': f'{running_loss/total:.4f}',\n            'acc': f'{100.*correct/total:.2f}%'\n        })\n    \n    epoch_loss = running_loss / total\n    epoch_acc = 100. * correct / total\n    return epoch_loss, epoch_acc\n\ndef validate(model, loader, criterion, device):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        pbar = tqdm(loader, desc='Validation')\n        for inputs, labels in pbar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            pbar.set_postfix({\n                'loss': f'{running_loss/total:.4f}',\n                'acc': f'{100.*correct/total:.2f}%'\n            })\n    \n    epoch_loss = running_loss / total\n    epoch_acc = 100. * correct / total\n    return epoch_loss, epoch_acc\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'lr': []\n}\n\n# Early stopping\nbest_val_acc = 0.0\npatience_counter = 0\nbest_model_path = os.path.join(CONFIG['output_dir'], 'best_model_brand.pth')\n\nprint(\"\\nStarting training...\\n\")\nprint(f\"{'Epoch':<6} {'Train Loss':<12} {'Train Acc':<12} {'Val Loss':<12} {'Val Acc':<12} {'LR':<12} {'Status'}\")\nprint(\"-\" * 90)\n\nfor epoch in range(CONFIG['num_epochs']):\n    # Train\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    \n    # Validate\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    \n    # Update scheduler\n    if CONFIG['scheduler_type'] == 'cosine':\n        scheduler.step()\n    else:\n        scheduler.step(val_acc)\n    \n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Save history\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['lr'].append(current_lr)\n    \n    # Check for improvement\n    status = \"\"\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': val_acc,\n            'class_names': class_names,\n            'history': history  # Save history in checkpoint\n        }, best_model_path)\n        status = \"✓ Saved\"\n    else:\n        patience_counter += 1\n        status = f\"Patience: {patience_counter}/{CONFIG['patience']}\"\n    \n    # Save history backup every epoch (for recovery if training is interrupted)\n    with open(os.path.join(CONFIG['output_dir'], 'training_history_brand.json'), 'w') as f:\n        json.dump(history, f)\n    \n    print(f\"{epoch+1:<6} {train_loss:<12.4f} {train_acc:<12.2f} {val_loss:<12.4f} {val_acc:<12.2f} {current_lr:<12.6f} {status}\")\n    \n    # Early stopping\n    if patience_counter >= CONFIG['patience']:\n        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n        break\n\nprint(\"\\n\" + \"=\"*90)\nprint(f\"Training completed! Best validation accuracy: {best_val_acc:.2f}%\")\nprint(f\"Best model saved to: {best_model_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Recover history if not in memory (e.g., after kernel restart or interruption)\nif 'history' not in locals() or not history or len(history.get('train_loss', [])) == 0:\n    print(\"History not found in memory, attempting to recover...\")\n    \n    # Try loading from checkpoint first\n    try:\n        if os.path.exists(best_model_path):\n            checkpoint = torch.load(best_model_path)\n            if 'history' in checkpoint:\n                history = checkpoint['history']\n                print(f\"✓ History recovered from checkpoint (epoch {checkpoint['epoch']+1})\")\n            else:\n                raise KeyError(\"No history in checkpoint\")\n        else:\n            raise FileNotFoundError(\"Checkpoint not found\")\n    except (FileNotFoundError, KeyError):\n        # Fallback to JSON file\n        try:\n            json_path = os.path.join(CONFIG['output_dir'], 'training_history_brand.json')\n            with open(json_path, 'r') as f:\n                history = json.load(f)\n            print(f\"✓ History recovered from JSON backup\")\n        except FileNotFoundError:\n            print(\"⚠️ No history found - cannot plot training curves\")\n            print(\"This usually happens if training was interrupted before completing any epochs.\")\n            history = None\n\n# Plot training history if available\nif history and len(history.get('train_loss', [])) > 0:\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    epochs_range = range(1, len(history['train_loss']) + 1)\n\n    # Loss\n    axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n    axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss', fontsize=12)\n    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    axes[0].legend(fontsize=11)\n    axes[0].grid(True, alpha=0.3)\n\n    # Accuracy\n    axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n    axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n    axes[1].axhline(y=85, color='g', linestyle='--', label='Target (85%)', linewidth=1.5)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n    axes[1].legend(fontsize=11)\n    axes[1].grid(True, alpha=0.3)\n\n    # Learning rate\n    axes[2].plot(epochs_range, history['lr'], 'g-', linewidth=2)\n    axes[2].set_xlabel('Epoch', fontsize=12)\n    axes[2].set_ylabel('Learning Rate', fontsize=12)\n    axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n    axes[2].set_yscale('log')\n    axes[2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(CONFIG['output_dir'], 'training_history_brand.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n\n    print(f\"\\nTraining curves saved to {CONFIG['output_dir']}/training_history_brand.png\")\nelse:\n    print(\"Skipping visualization - no training history available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Model Evaluation and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model\ncheckpoint = torch.load(best_model_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\nprint(f\"Best validation accuracy: {checkpoint['val_acc']:.2f}%\")\n\n# Evaluate on test set\nif test_dataset:\n    print(\"\\nEvaluating on test set...\")\n    test_loss, test_acc = validate(model, test_loader, criterion, device)\n    print(f\"\\nTest Results:\")\n    print(f\"  Test Loss: {test_loss:.4f}\")\n    print(f\"  Test Accuracy: {test_acc:.2f}%\")\nelse:\n    print(\"\\nNo test set available - using validation results as final metrics\")\n    test_acc = checkpoint['val_acc']\n\n# Export to ONNX\nprint(\"\\nExporting model to ONNX format...\")\ndummy_input = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(device)\nonnx_path = os.path.join(CONFIG['output_dir'], 'car_brand_classifier.onnx')\n\ntorch.onnx.export(\n    model,\n    dummy_input,\n    onnx_path,\n    export_params=True,\n    opset_version=12,\n    do_constant_folding=True,\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n)\n\nprint(f\"ONNX model saved to: {onnx_path}\")\nprint(f\"  Model size: {os.path.getsize(onnx_path) / (1024**2):.2f} MB\")\n\n# Save class labels\nlabels_path = os.path.join(CONFIG['output_dir'], 'class_labels_brand.json')\nwith open(labels_path, 'w') as f:\n    json.dump({'classes': class_names}, f, indent=2)\n\nprint(f\"Class labels saved to: {labels_path}\")\nprint(f\"\\nNumber of brands: {len(class_names)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Grad-CAM Implementation\n",
    "\n",
    "Implement Grad-CAM to visualize which parts of the image the model focuses on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Grad-CAM\n",
    "target_layers = [model.conv_head]  # Last convolutional layer before classification\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "print(\"Grad-CAM initialized\")\n",
    "print(f\"Target layer: conv_head (final convolutional layer)\")\n",
    "print(f\"This layer captures high-level features before the classifier\")\n",
    "\n",
    "def generate_gradcam(model, img_tensor, true_label, pred_label, cam, class_names):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM visualization for a single image.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        img_tensor: Image tensor (C, H, W) normalized\n",
    "        true_label: Ground truth label index\n",
    "        pred_label: Predicted label index\n",
    "        cam: GradCAM instance\n",
    "        class_names: List of class names\n",
    "    \n",
    "    Returns:\n",
    "        original_img: Denormalized original image\n",
    "        cam_visualization: Grad-CAM heatmap overlay\n",
    "        grayscale_cam: Raw grayscale Grad-CAM\n",
    "    \"\"\"\n",
    "    # Denormalize image for visualization\n",
    "    img_denorm = denormalize(img_tensor.cpu()).permute(1, 2, 0).numpy()\n",
    "    img_denorm = np.clip(img_denorm, 0, 1)\n",
    "    \n",
    "    # Generate Grad-CAM for predicted class\n",
    "    targets = [ClassifierOutputTarget(pred_label.item())]\n",
    "    grayscale_cam = cam(input_tensor=img_tensor.unsqueeze(0), targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Create visualization\n",
    "    visualization = show_cam_on_image(img_denorm, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    return img_denorm, visualization, grayscale_cam\n",
    "\n",
    "print(\"\\nGrad-CAM helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Visualize Correct Predictions with Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correctly predicted samples\n",
    "model.eval()\n",
    "correct_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(min(100, len(val_dataset))):\n",
    "        img, label = val_dataset[i]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        if pred.item() == label:\n",
    "            correct_samples.append((i, img, label, pred, output))\n",
    "        \n",
    "        if len(correct_samples) >= 12:\n",
    "            break\n",
    "\n",
    "# Visualize correct predictions with Grad-CAM\n",
    "fig, axes = plt.subplots(4, 6, figsize=(20, 14))\n",
    "fig.suptitle('Correct Predictions with Grad-CAM Visualization', fontsize=16, y=0.995)\n",
    "\n",
    "for idx in range(min(12, len(correct_samples))):\n",
    "    i, img, label, pred, output = correct_samples[idx]\n",
    "    \n",
    "    # Get confidence\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    confidence = probs[0, pred].item() * 100\n",
    "    \n",
    "    # Generate Grad-CAM\n",
    "    img_denorm, cam_vis, _ = generate_gradcam(model, img.to(device), label, pred, cam, class_names)\n",
    "    \n",
    "    # Plot original\n",
    "    row = (idx // 3) * 2\n",
    "    col = (idx % 3) * 2\n",
    "    axes[row, col].imshow(img_denorm)\n",
    "    axes[row, col].set_title(f\"Original\\n{class_names[label]}\", fontsize=9)\n",
    "    axes[row, col].axis('off')\n",
    "    \n",
    "    # Plot Grad-CAM\n",
    "    axes[row, col+1].imshow(cam_vis)\n",
    "    axes[row, col+1].set_title(f\"Grad-CAM\\nConf: {confidence:.1f}%\", fontsize=9)\n",
    "    axes[row, col+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'gradcam_correct_predictions_brand.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Grad-CAM visualizations for correct predictions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Visualize Incorrect Predictions with Grad-CAM (Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incorrectly predicted samples\n",
    "incorrect_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        img, label = val_dataset[i]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        if pred.item() != label:\n",
    "            incorrect_samples.append((i, img, label, pred, output))\n",
    "        \n",
    "        if len(incorrect_samples) >= 12:\n",
    "            break\n",
    "\n",
    "if len(incorrect_samples) > 0:\n",
    "    # Visualize incorrect predictions with Grad-CAM\n",
    "    fig, axes = plt.subplots(4, 6, figsize=(20, 14))\n",
    "    fig.suptitle('Incorrect Predictions with Grad-CAM (Debugging)', fontsize=16, y=0.995)\n",
    "    \n",
    "    for idx in range(min(12, len(incorrect_samples))):\n",
    "        i, img, label, pred, output = incorrect_samples[idx]\n",
    "        \n",
    "        # Get confidence\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        confidence = probs[0, pred].item() * 100\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        img_denorm, cam_vis, _ = generate_gradcam(model, img.to(device), label, pred, cam, class_names)\n",
    "        \n",
    "        # Plot original\n",
    "        row = (idx // 3) * 2\n",
    "        col = (idx % 3) * 2\n",
    "        axes[row, col].imshow(img_denorm)\n",
    "        axes[row, col].set_title(f\"True: {class_names[label]}\", fontsize=9, color='green')\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        # Plot Grad-CAM\n",
    "        axes[row, col+1].imshow(cam_vis)\n",
    "        axes[row, col+1].set_title(f\"Pred: {class_names[pred.item()]}\\nConf: {confidence:.1f}%\", fontsize=9, color='red')\n",
    "        axes[row, col+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'gradcam_incorrect_predictions_brand.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Found {len(incorrect_samples)} incorrect predictions\")\n",
    "    print(f\"Grad-CAM visualizations for incorrect predictions saved\")\n",
    "else:\n",
    "    print(\"Perfect validation accuracy - no incorrect predictions found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 15: Grad-CAM Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"Grad-CAM Analysis Summary\")\nprint(\"=\"*70)\nprint(\"\\nWhat Grad-CAM Shows:\")\nprint(\"  - Red/Yellow regions = Areas the model focuses on most\")\nprint(\"  - Blue/Purple regions = Areas the model ignores\")\nprint(\"\\nFor Car Brand Classification:\")\nprint(\"  - Model should focus on distinctive features:\")\nprint(\"    - Front grille shape and logo\")\nprint(\"    - Headlight design\")\nprint(\"    - Body shape and proportions\")\nprint(\"    - Badge and emblems\")\nprint(\"\\nUse Case in Production:\")\nprint(\"  1. Build trust - Show users why the model made a decision\")\nprint(\"  2. Debugging - Identify if model is looking at wrong features\")\nprint(\"  3. Insurance validation - Verify model focuses on car features\")\nprint(\"  4. User interface - Display heatmap alongside predictions\")\nprint(\"\\nNext Steps:\")\nprint(\"  - Integrate Grad-CAM into API response\")\nprint(\"  - Display heatmaps in web application\")\nprint(\"  - Use for debugging edge cases and failures\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16: Generate Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Generate predictions for test/validation set\n",
    "eval_loader = test_loader if test_dataset else val_loader\n",
    "eval_dataset = test_dataset if test_dataset else val_dataset\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(eval_loader, desc='Generating predictions'):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix (simplified for many classes)\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - Car Brand Classification\\n(Test Accuracy: {test_acc:.2f}%)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Brand', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Brand', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'confusion_matrix_brand.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, digits=3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "with open(os.path.join(CONFIG['output_dir'], 'classification_report_brand.txt'), 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nConfusion matrix saved to {CONFIG['output_dir']}/confusion_matrix_brand.png\")\n",
    "print(f\"Classification report saved to {CONFIG['output_dir']}/classification_report_brand.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 17: Final Summary and Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"CAR BRAND CLASSIFICATION - TRAINING COMPLETE\")\nprint(\"=\"*70)\n\nprint(\"\\nPERFORMANCE METRICS:\")\nprint(f\"  - Best Validation Accuracy: {best_val_acc:.2f}%\")\nprint(f\"  - Test Accuracy: {test_acc:.2f}%\")\nprint(f\"  - Number of Brands: {num_classes}\")\nprint(f\"  - Training Epochs: {len(history['train_loss'])}\")\nprint(f\"  - Model: {CONFIG['model_name']}\")\n\nprint(\"\\nMODEL FILES (Download these from Kaggle):\")\nprint(f\"  1. {CONFIG['output_dir']}/car_brand_classifier.onnx\")\nprint(f\"     → ONNX model for deployment\")\nprint(f\"  2. {CONFIG['output_dir']}/class_labels_brand.json\")\nprint(f\"     → Brand name mappings\")\nprint(f\"  3. {CONFIG['output_dir']}/best_model_brand.pth\")\nprint(f\"     → PyTorch checkpoint (backup)\")\n\nprint(\"\\nVISUALIZATION FILES:\")\nprint(f\"  4. {CONFIG['output_dir']}/training_history_brand.png\")\nprint(f\"  5. {CONFIG['output_dir']}/confusion_matrix_brand.png\")\nprint(f\"  6. {CONFIG['output_dir']}/gradcam_correct_predictions_brand.png\")\nprint(f\"  7. {CONFIG['output_dir']}/gradcam_incorrect_predictions_brand.png\")\nprint(f\"  8. {CONFIG['output_dir']}/classification_report_brand.txt\")\n\nprint(\"\\nNEXT STEPS:\")\nprint(\"  1. Download all model files from /kaggle/working\")\nprint(\"  2. Move to project: models/final/\")\nprint(\"  3. Prepare deployment with both models:\")\nprint(\"     - car_body_type_classifier (7 body types)\")\nprint(\"     - car_brand_classifier (33 brands)\")\nprint(\"  4. Build web application with dual classification + Grad-CAM\")\nprint(\"  5. Integrate with insurance premium calculator\")\n\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}